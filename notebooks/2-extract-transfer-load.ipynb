{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract, transfer, and load raw PDF data\n",
    "\n",
    "This notebook seeks to replicate the functionality of the R package `retrieveR` in a Python 3 environment. It performs about 80% (subjectively) as well as the R counterpart, with additional casing needed for certain types of documents.\n",
    "\n",
    "**NOTE**: Use R counterpart for now. This is not production ready, and other sections of pipeline use `retrieveR`.\n",
    "\n",
    "Last Updated: July 23, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.restart({kernel_name: 'policy-toolkit'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from wand.image import Image as wimage\n",
    "import os\n",
    "import io\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "import re\n",
    "from itertools import islice\n",
    "from functools import reduce\n",
    "import string\n",
    "\n",
    "filepath = \"../data/raw/Forest Conservation and Management Act.pdf\"\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, path, titles = None):\n",
    "        self.path = path\n",
    "        self.name = self.path\n",
    "    \n",
    "    def get_images(self):\n",
    "        page_images = []\n",
    "        with wimage(filename=self.path, resolution=200) as img:\n",
    "            for page_wand_image_seq in img.sequence:\n",
    "                page_wand_image = wimage(page_wand_image_seq)\n",
    "                page_jpeg_bytes = page_wand_image.make_blob(format=\"jpeg\")\n",
    "                page_jpeg_data = io.BytesIO(page_jpeg_bytes)\n",
    "                page_image = Image.open(page_jpeg_data)\n",
    "                page_images.append(page_image)\n",
    "        self.images = page_images\n",
    "        \n",
    "    def process_images(self):\n",
    "        self.text = np.array([pytesseract.image_to_string(x) for x in self.images])\n",
    "        self.pages = [x for x in range(len(self.images))]\n",
    "        \n",
    "    def split_lines(self):\n",
    "        self.lines = {}\n",
    "        counter = 0\n",
    "        pages = [re.split('\\n', self.text[i]) for i in self.pages]\n",
    "        for x, page in enumerate(pages):\n",
    "            for i, line in enumerate(page):\n",
    "                self.lines[i + counter] = [line, x]\n",
    "            counter += len(page)\n",
    "    \n",
    "    def remove_titles(self):\n",
    "        titles = [x for x in list(self.lines.keys()) if self.lines[x][0].lower() != self.lines[x][0] and\\\n",
    "                  len(self.lines[x][0]) <= 50]\n",
    "        titles = [x for x in titles if self.lines[max(0, x - 1)][0].endswith('.') or\\\n",
    "                  self.lines[max(0, x - 1)][0] == '']\n",
    "        print(\"Removing {} titles\".format(len(titles)))\n",
    "        for x in titles:\n",
    "            del self.lines[x]\n",
    "        self.lines = { key : value for key, value in zip(range(len(self.lines)), self.lines.values())}\n",
    "        \n",
    "    def combine_sentences(self):\n",
    "        self.paragraphs = []\n",
    "        ends = [0] + [i + 1 for i in self.lines.keys() if self.lines.get(i)[0].endswith('.')]\n",
    "        sentence_ids = [(val, ends[x + 1]) for x, val in enumerate(ends) if val < max(ends)]\n",
    "        for i in sentence_ids:\n",
    "            lines = [self.lines.get(x)[0] for x in range(i[0], i[1])]\n",
    "            self.paragraphs.append(' '.join(' '.join(lines).split()))\n",
    "            \n",
    "    def remove_headers(self):\n",
    "        def _window(seq, n):\n",
    "            \"Returns a sliding window (of width n) over data from the iterable\"\n",
    "            \"   s -> (s0,s1,...s[n-1]), (s1,s2,...,sn), ...                   \"\n",
    "            it = iter(seq)\n",
    "            result = tuple(islice(it, n))\n",
    "            if len(result) == n:\n",
    "                yield result\n",
    "            for elem in it:\n",
    "                result = result[1:] + (elem,)\n",
    "                yield result\n",
    "\n",
    "\n",
    "        words = [x.replace(\".pdf\", \"\") for x in self.path.replace(\"/\", \" \").split(\" \") if x not in [\"..\", \"data\", \"raw\"]]\n",
    "\n",
    "        locs = []\n",
    "        for i in reversed(range(3, len(words) + 1)):\n",
    "            for x in window(words, n = i):\n",
    "                locs.append(' '.join(x))\n",
    "\n",
    "        self.clean = [reduce(lambda item, loc: item.replace(loc,''), [item]+locs)\n",
    "            for item in self.paragraphs]\n",
    "        \n",
    "    def clean_data(self):\n",
    "        self.clean = [re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x).lower() for x in self.clean]\n",
    "        self.clean = [x.translate(str.maketrans('', '', string.punctuation)) for x in self.clean]\n",
    "        \n",
    "    def export_data(self):\n",
    "        self.export_path = self.path.replace(\"raw\", \"processed\")\n",
    "        self.export_path = self.export_path.replace(\".pdf\", \".txt\")\n",
    "        with open(self.export_path, \"w\") as text_file:\n",
    "            for x in self.clean:\n",
    "                text_file.write(x)\n",
    "                text_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DRAFT ASAL POLICY.pdf',\n",
       " 'Charcoal Rules.pdf',\n",
       " 'National Land Policy.pdf',\n",
       " 'Constitution.pdf',\n",
       " 'Agriculture Rules.pdf',\n",
       " 'Community Land Act.pdf',\n",
       " 'National Climate Responses Strategy.pdf',\n",
       " 'Agriculture Sectoral Development Strategy.pdf',\n",
       " 'Forest Conservation and Management Act.pdf',\n",
       " 'Vision 2030.pdf',\n",
       " 'Fifth Report to Conference of Parties.pdf',\n",
       " 'The Environmental Management and Coordination Act.pdf']"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepaths = os.listdir(\"../data/raw/\")\n",
    "filepaths = [x for x in filepaths if x[-4:] == \".pdf\"]\n",
    "filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in filepaths:\n",
    "    print(i)\n",
    "    doc1 = Document(path = \"../data/raw/\" + i)\n",
    "    doc1.get_images()\n",
    "    doc1.process_images()\n",
    "    print(\"Images processed\")\n",
    "    doc1.split_lines()\n",
    "    doc1.remove_titles()\n",
    "    doc1.combine_sentences()\n",
    "    print(\"Sentences combined\")\n",
    "    doc1.remove_headers()\n",
    "    doc1.clean_data()\n",
    "    print(\"Text cleaned\")\n",
    "    doc1.export_data()\n",
    "    print(\"Exported data to: {}\".format(doc1.export_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "policy-toolkit",
   "language": "python",
   "name": "policy-toolkit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
