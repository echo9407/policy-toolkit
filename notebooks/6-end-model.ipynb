{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End model to predict financial incentives and disincentives\n",
    "\n",
    "John Brandt\n",
    "\n",
    "Last updated: Aug 19, 2019\n",
    "\n",
    "\n",
    "This notebook contains a gold standard baseline (LSTM with gold standard labels) as well as a noisy implementation of snorkel labels with roBERTa encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, list_IDs, labels, mode):\n",
    "        'Initialization'\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        ID = self.list_IDs[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        X = np.load('../data/processed/' + self.mode + '_embeddings/' + str(ID) + '.npy')\n",
    "        y = self.labels[ID]\n",
    "\n",
    "        return X.reshape((50, 1024)), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/processed/gold_standard.csv\")\n",
    "gs_probas = np.load('../data/interim/snorkel_proba.npy')\n",
    "noisy_probas = np.load('../data/interim/snorkel_noisy_proba.npy')\n",
    "\n",
    "params = {'batch_size': 50,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 2}\n",
    "\n",
    "Y = df['class'] - 1\n",
    "    \n",
    "# Datasets\n",
    "partition = {'train': [x for x in range(0, 800)],\n",
    "             'validation': [x for x in range(800, 1000)]}\n",
    "\n",
    "noisy_labels = {k:w for w,k in zip(noisy_probas, range(1000))}\n",
    "gs_labels = {k:w for w,k in zip(Y, range(800, 1000))}\n",
    "\n",
    "# Generators\n",
    "training_set = Dataset(partition['train'], gs_probas[0:800], 'test')\n",
    "training_generator = data.DataLoader(training_set, **params)\n",
    "\n",
    "validation_set = Dataset(partition['validation'], gs_labels, 'test')\n",
    "validation_generator = data.DataLoader(validation_set, **params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Soft label loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SoftCrossEntropyLoss(nn.Module):\n",
    "    \"\"\"Computes the CrossEntropyLoss while accepting probabilistic (float) targets\n",
    "    Args:\n",
    "        weight: a tensor of relative weights to assign to each class.\n",
    "            the kwarg name 'weight' is used to match CrossEntropyLoss\n",
    "        reduction: how to combine the elementwise losses\n",
    "            'none': return an unreduced list of elementwise losses\n",
    "            'mean': return the mean loss per elements\n",
    "            'sum': return the sum of the elementwise losses\n",
    "    Accepts:\n",
    "        input: An [n, k] float tensor of prediction logits (not probabilities)\n",
    "        target: An [n, k] float tensor of target probabilities\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weight=None, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "        # Register as buffer is standard way to make sure gets moved /\n",
    "        # converted with the Module, without making it a Parameter\n",
    "        if weight is None:\n",
    "            self.weight = None\n",
    "        else:\n",
    "            # Note: Sets the attribute self.weight as well\n",
    "            self.register_buffer(\"weight\", torch.FloatTensor(weight))\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        n, k = input.shape\n",
    "        # Note that t.new_zeros, t.new_full put tensor on same device as t\n",
    "        cum_losses = input.new_zeros(n)\n",
    "        for y in range(k):\n",
    "            cls_idx = input.new_full((n,), y, dtype=torch.long)\n",
    "            y_loss = F.cross_entropy(input, cls_idx, reduction=\"none\")\n",
    "            if self.weight is not None:\n",
    "                y_loss = y_loss * self.weight[y]\n",
    "            cum_losses += target[:, y].float() * y_loss\n",
    "        if self.reduction == \"none\":\n",
    "            return cum_losses\n",
    "        elif self.reduction == \"mean\":\n",
    "            return cum_losses.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return cum_losses.sum()\n",
    "        else:\n",
    "            raise ValueError(f\"Unrecognized reduction: {self.reduction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply(model, criterion, batch, targets):\n",
    "    pred = model(torch.autograd.Variable(batch))\n",
    "    loss = criterion(pred, torch.autograd.Variable(targets))\n",
    "    return pred, loss\n",
    "\n",
    "\n",
    "def train(model,\n",
    "         n_epochs,\n",
    "         train_generator,\n",
    "         val_generator):\n",
    "    \n",
    "    softcriterion = SoftCrossEntropyLoss()\n",
    "    hardcriterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.002)\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        for inputs, labels in train_generator:\n",
    "            counter += 1\n",
    "            model.zero_grad()\n",
    "            inputs = inputs.type(torch.FloatTensor)\n",
    "            pred, loss = apply(model, softcriterion, inputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = model.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                model.eval()\n",
    "                for inputs, labels in val_generator:\n",
    "                    inputs = inputs.type(torch.FloatTensor)\n",
    "                    output = model(inputs)\n",
    "                    val_loss = hardcriterion(output.reshape((batch_size, 3)), labels.long())\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                model.train()\n",
    "                print(\"Epoch: {}/{}...\".format(epoch+1, n_epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold standard baseline\n",
    "\n",
    "Shallow RNN with RoBERTa encoded words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.autograd as autograd\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Defining the layers\n",
    "        # RNN Layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, self.output_size)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        self.hidden = self.init_hidden(batch.size(-2))\n",
    "        outputs, (ht, ct) = self.lstm(batch, self.hidden)\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = self.fc(ht[-1])\n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return(autograd.Variable(torch.randn(1, batch_size, self.hidden_dim)),\n",
    "                                 autograd.Variable(torch.randn(1, batch_size, self.hidden_dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_size = 1024, output_size = 3, hidden_dim = 200, n_layers = 1)\n",
    "model.to(\"cpu\")\n",
    "\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "print_every = 100\n",
    "batch_size = 50\n",
    "\n",
    "model = train(model = model,\n",
    "             n_epochs = 250,\n",
    "             train_generator = training_generator,\n",
    "             val_generator = validation_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snorkel end model with RoBERTA-LSTM input module and noise-aware output head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "device = \"cpu\"\n",
    "print_every = 100\n",
    "batch_size = 50\n",
    "\n",
    "criterion = SoftCrossEntropyLoss(reduction = 'mean')\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    h = model.init_hidden(batch_size)\n",
    "    for inputs, labels in training_generator:\n",
    "        counter += 1\n",
    "        h = tuple([each.data for each in h])\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.type(torch.FloatTensor)\n",
    "        output, h = model(inputs)\n",
    "        loss = criterion(output.reshape((batch_size, 3)), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = model.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for inputs, labels in validation_generator:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                inputs = inputs.type(torch.FloatTensor)\n",
    "                output, val_h = model(inputs)\n",
    "                val_loss = criterion(output.reshape((batch_size, 3)), labels)\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(epoch+1, n_epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do: Snorkel end model with RoBERTa-LSTM input module and multi-task output head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do: Snorkel end model with RoBERTa-LSTM input module and multi-task output head, with concatenation of additional feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do: Snorkel end model with RoBERTa-LSTM input module and multi-task output head, with concatenation of additional feature engineering and synonym augmentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "policy-toolkit",
   "language": "python",
   "name": "policy-toolkit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
